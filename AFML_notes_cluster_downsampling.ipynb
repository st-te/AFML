{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and Downsampling: Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean shift clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Shift Clustering:**\n",
    "\n",
    "**Idea:** Mean Shift identifies clusters by iteratively shifting each data point towards the mode (peak) of the data's density distribution.\n",
    "\n",
    "**Pros:** It doesn't require specifying the number of clusters in advance and can discover clusters of arbitrary shapes.\n",
    "\n",
    "**Cons:** It can be computationally intensive, and the algorithm's performance depends on the choice of bandwidth parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under mean shift clustering:**\n",
    "\n",
    "bandwidth=0.00000475 found to be suitable for THIS instance of AFM data\n",
    "\n",
    "bandwidth=0.000005 eradicated level2 terrace in sample data\n",
    "\n",
    "bandwidth<0.00000475 led to monatomic-width pseudolevels between terraces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular grid interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation captures finer details and curved edges by estimating data values at positions within the original dataset based on the known data points. Here's how it works:\n",
    "\n",
    "**Interpolation Methods:**\n",
    "\n",
    "When you use interpolation, you essentially create a mathematical model or function that approximates the data values between the original data points.\n",
    "Different interpolation methods, such as linear, cubic, or spline interpolation, use varying degrees of mathematical complexity to estimate the values.\n",
    "These methods can capture intricate patterns, including curved edges, by fitting a smooth curve or surface through the data points.\n",
    "\n",
    "**Higher Resolution:**\n",
    "\n",
    "By defining a new grid with a higher resolution (more points) for interpolation, you increase the density of estimation points.\n",
    "This higher density of points allows the interpolation method to capture finer details in the data, such as subtle variations and curved shapes.\n",
    "\n",
    "**Local Detail Preservation:**\n",
    "\n",
    "Many interpolation methods, especially spline-based techniques, excel at preserving local detail, including curvature and sharp changes in the data.\n",
    "They create a smooth transition between known data points, which helps maintain the integrity of features like curved edges.\n",
    "\n",
    "While interpolation can be effective for preserving details, it's important to consider its limitations:\n",
    "\n",
    "**Advantages of Interpolation:**\n",
    "\n",
    "Preserves fine details and features, including curved edges.\n",
    "Allows for high-resolution downsampling when applied to a denser grid.\n",
    "Relatively straightforward to implement and use with scikit-learn's RegularGridInterpolator.\n",
    "\n",
    "**Disadvantages of Interpolation:**\n",
    "\n",
    "Can introduce artificial smoothness: Some interpolation methods might oversmooth the data, potentially reducing the ability to capture sharp features.\n",
    "Sensitive to grid resolution: The effectiveness of interpolation depends on the density of the grid, and too high a resolution might lead to overfitting.\n",
    "May not handle outliers well: Interpolation methods can be sensitive to outliers or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE (t-distributed stochastic neighbor embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE (t-distributed stochastic neighbor embedding) is an unsupervised dimensionality reduction technique that doesn't require labeled training data. Instead, it operates on the input data directly. Here's how it works:\n",
    "\n",
    "**Data Structure Preservation:** t-SNE tries to map high-dimensional data points to a lower-dimensional space while preserving the pairwise similarity structure of the data as much as possible. It's effective at capturing clusters, local structures, and non-linear relationships.\n",
    "\n",
    "**Parameter Tuning:** t-SNE has a few parameters, such as the perplexity, which you can adjust to control the trade-off between preserving local and global structures. The optimal perplexity value often depends on the specific dataset and use case.\n",
    "\n",
    "**No Pre-trained Models:** It's designed to work directly with your data, making it suitable for a wide range of applications.\n",
    "\n",
    "**Computational Cost:** t-SNE can be computationally expensive, especially for large datasets. However, there are approximate algorithms like \"MulticoreTSNE\" that can speed up the process.\n",
    "\n",
    "Regarding the amount of data needed, t-SNE can be effective with relatively small to moderate-sized datasets. It doesn't require a large amount of data, but its performance can improve with more data points, especially when you want to capture subtle data relationships and structures.\n",
    "\n",
    "If you have a specific application where you want to use t-SNE to visualize or reduce the dimensionality of your data, you can start with a reasonable subset of your AFM data to see how t-SNE performs. You can then adjust the perplexity and other parameters to fine-tune the results.\n",
    "\n",
    "You can find various implementations of t-SNE in Python libraries like scikit-learn and MulticoreTSNE, which provide straightforward ways to apply t-SNE to your data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
